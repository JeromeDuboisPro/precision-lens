# PrecisionLens

**Visualizing precision-performance tradeoffs in numerical computing**

Modern AI accelerators like NVIDIA's Hopper architecture leverage reduced floating-point precision (FP8/FP16) for massive throughput gains. But when do numerical algorithms break? PrecisionLens provides interactive visualizations to explore this frontier.

## ğŸ¯ Interactive Dashboard

**[Try the live demo â†’](#)** *(Deploy to GitHub Pages)*

Watch the power method algorithm converge across FP64, FP32, FP16, and FP8 in real-time:
- âš¡ Side-by-side convergence comparison
- ğŸ“Š Performance metrics (FLOPS, memory bandwidth)
- ğŸ® Interactive replay controls
- ğŸ“ˆ Auto-generated insights

![Dashboard Preview](web/assets/preview.png)

## ğŸš€ Quick Start

### View Interactive Dashboard

```bash
cd web
python3 -m http.server 8000
# Open http://localhost:8000
```

### Run Static Analysis

```bash
pip install -r requirements.txt
python algorithms/power_method/study.py
```

### Generate New Traces

```bash
python scripts/generate_traces.py
```

## ğŸ“Š Key Findings

Using the power method for eigenvalue computation (matrix size: 50):

| Precision | Speedup | Final Error | Use Case |
|-----------|---------|-------------|----------|
| **FP64** | 1.0Ã— | ~10â»Â¹â° | Reference baseline |
| **FP32** | 4-5Ã— | ~10â»â¶ | Most scientific computing |
| **FP16** | 20Ã— | ~10â»Â² | ML training, well-conditioned problems |
| **FP8** | 70-80Ã— | ~10-20% | ML inference with error tolerance |

**Insight**: FP32 achieves 4-5Ã— speedup with negligible accuracy loss for most applications. FP8 shows dramatic speedup but requires careful validation.

## ğŸ› ï¸ Project Structure

```
precision-lens/
â”œâ”€â”€ algorithms/power_method/
â”‚   â”œâ”€â”€ study.py           # Original batch analysis
â”‚   â”œâ”€â”€ instrumented.py    # Detailed performance tracing
â”‚   â””â”€â”€ traces/            # Generated execution traces
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ generate_traces.py # Batch trace generation
â”œâ”€â”€ web/                    # Interactive dashboard
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ dashboard.js
â”‚   â””â”€â”€ traces/            # Pre-generated data
â””â”€â”€ results/               # Static plots
```

## ğŸ’¡ Educational Applications

Perfect for:
- Understanding mixed-precision training/inference
- Teaching numerical stability concepts
- Demonstrating hardware-algorithm co-design
- Portfolio projects for ML/HPC roles

## ğŸ“ Technical Details

**Algorithm**: Power method for dominant eigenvalue computation

**Precision Formats**:
- FP64: IEEE 754 double precision
- FP32: IEEE 754 single precision
- FP16: IEEE 754 half precision
- FP8: Simulated via mantissa quantization

**Performance Metrics**:
- Theoretical FLOPS: 2nÂ² + n operations per iteration
- Memory bandwidth: (nÂ² + 2n) Ã— bytes per element
- Convergence: Relative error vs iteration count

## ğŸ”— Relevant Context

Modern AI accelerators:
- **NVIDIA H100**: 2000 TFLOPS (FP8) vs 60 TFLOPS (FP16)
- **AMD MI300X**: 1300 TFLOPS (FP8) vs 82 TFLOPS (FP16)
- **Precision choice = critical performance factor**

This tool demonstrates these tradeoffs interactively.

## ğŸ“œ License

MIT

---

*Built to explore the precision-performance frontier shaping modern AI hardware*
